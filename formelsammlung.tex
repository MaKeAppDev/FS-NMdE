% Formelsammlung Numerische Methoden der Elektrotechnik
%
% Geschrieben im SS 2014 an der TU München
% von Markus Hofbauer, Kevin Meyer und Benedikt Schmidt für LaTeX4EI (latex4ei.de)
% Kontakt: latex@kevin-meyer.de oder via Kontaktformular auf http://latex4ei.de

% Dokumenteinstellungen
% ======================================================================


% Dokumentklasse (Schriftgröße 6, DIN A4, Artikel)
\documentclass[german]{latex4ei/latex4ei_sheet}

\usepackage[european]{circuitikz}
\usepackage{tabularx}
\usepackage{multirow}
\usetikzlibrary{arrows, calc, intersections}
\usepackage{calc}

% Für code
\definecolor{COMMENTGREEN}{HTML}{228B22}
\definecolor{MATLABBACKGROUND}{HTML}{FCFCDC}
\lstset{ %
language=Matlab,						% choose the language of the code
%basicstyle=\ttfamily,					% the size of the fonts that are used for the code
%emphstyle=\color{yellow}\ttfalily,
%keywordstyle=\color{blue}\ttfamily,
%stringstyle=\color{magenta}\ttfamily,
%commentstyle=\color{COMMENTGREEN}\ttfamily,
%xleftmargin=10.3pt,						% distance to margin left
%xrightmargin=-3pt,						% distance to margin right
%linewidth=\widthof{\begin{sectionbox}}}		% this would be better instead of left and right margin
%aboveskip=0.6\baselineskip,
%belowskip=0\baselineskip,
numbers=left,                   		% where to put the line-numbers
%framexleftmargin=1.5em,					% distance to xleftmargin
%numberstyle=\ttfamily\footnotesize,		% the size of the fonts that are used for the line-numbers
%stepnumber=1,							% the step between two line-numbers. If it is 1 each line will be numbered
%numbersep=5pt,							% how far the line-numbers are from the code
%backgroundcolor=\color{MATLABBACKGROUND},	% choose the background color. You must add \usepackage{color}
%showspaces=false,						% show spaces adding particular underscores
%showstringspaces=false,					% underline spaces within strings
%showtabs=false,							% show tabs within strings adding particular underscores
%frame=single,							% adds a frame around the code (Box) (Für top und bottom rule set option to "lines")
%rulecolor=\color{gray},					% color of framebox rule
%tabsize=4,								% sets default tabsize to 2 spaces
%captionpos=b,							% sets the caption-position to bottom
breaklines=true,						% sets automatic line breaking
breakatwhitespace=false,				% sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}					% if you want to add a comment within your code
}

% tabularx definition
\newcolumntype{C}{>{\centering\arraybackslash}X}
\newcolumntype{L}{@{\extracolsep\fill}X}

% SI-Zahlen mit Komma als Dezimaltrenner
\sisetup{locale=DE}

% Eigener Inhalt für Center-Teil des Footers
\title{Numerische Methoden der Elektrotechnik}
\author{von Markus Hofbauer, Kevin Meyer und Benedikt Schmidt - Kontakt:  \href{mailto:latex@kevin-meyer.de}{\textit{latex@kevin-meyer.de}}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\cond}{cond}
\DeclareMathOperator{\rang}{rang}

\DeclareMathOperator{\Bild}{Bild}
\DeclareMathOperator{\defect}{def}


% Dokumentbeginn
% ======================================================================
\begin{document}

%\IfFileExists{git.id}{\input{git.id}}{}
%\ifdefined\GitRevision\fancyfoot[R]{Stand: \GitNiceDate \ (git \GitRevision) \qquad \thepage}\fi

% Aufteilung in Spalten

\maketitle

\section{Grundlagen}
\begin{sectionbox}
\begin{tablebox}{@{\extracolsep\fill}lp{5cm}@{}}
	Numerik & liefert eine zahlenmäßige Lösung eines Problems mit einem Algorithmus.\\
	Kondition & Ein Maß wie stark sich Eingabefehler auf die Ausgabe auswirken. $\kappa = \frac{\norm{\delta f}}{\norm{\delta x}} \ra |f'(x)|$\\
	$f(x)$ & Mathematisches Problem $f$ mit exakter Eingabe $x$\\
	$\tilde f(\tilde x)$ & Numerischer Algorithmus $\tilde f$ mit gerundeter Eingabe $\tilde x$\\
\end{tablebox}
\end{sectionbox}

\begin{sectionbox}
\subsection{Spektralradius}
	\textbf{Spektralradius $\rho(\ma A)$ einer Matrix $\ma A$:} Betragsmäßig größter Eigenwert. \\
	Konvergenzbeweis aller Verfahren: Gershgorinkreise um die Null mit $r \le 1$
	\begin{equation*}
		\rho(\ma{A}) = \max_i \abs{\lambda_i}
	\end{equation*}

\subsection{Diagonaldominanz}
Diagonalelemente sind größer als die restlichen Elemente der selben Zeile:\\
\begin{equation*}
\ma{A} = (a_{ij}) \begin{aligned} \text{diagonaldominant} \\ \text{strikt diagonaldominant}\end{aligned} \Leftrightarrow \abs{a_{ii}} \begin{aligned} \ge \\ > \end{aligned} \sum\limits_{j = 1, j \ne i}^n \abs{a_{ij}}\ \forall i
\end{equation*}

\subsection{Definitheit}
\begin{equation*}
\ma{A} \begin{aligned} \text{positiv definit} \\ \text{ positiv semidefinit}\end{aligned} \Leftrightarrow \lambda_i \begin{aligned} > \\ \ge \end{aligned} 0\ \forall i \qquad\text{bzw.}\qquad\vec{x}^T \ma{A} \vec{x} \begin{aligned} > \\ \ge \end{aligned} 0\ \forall \vec{x} \ne 0
\end{equation*}
\end{sectionbox}

\begin{sectionbox}
	\subsection{Kondition}
	Verstärkung von Eingangsfehler im ungünstigsten Fall.
	\begin{symbolbox}
		\begin{tabularx}{\columnwidth}{lXlX}
			$y = f(x)$ & Unbekannte & $x$ & Problemdaten
		\end{tabularx}
	\end{symbolbox}
	$\kappa_{\ir abs}(x) = \abs{f'(x)}$ \qquad\qquad $\kappa_{\ir rel}(x) = \abs{\frac{f'(x)}{\frac{f(x)}{x}}} = \frac{\abs{f'(x)} \cdot \abs{x}}{\abs{f(x)}}$\\
	Falls $\kappa_{\ir rel} \ll 100$: gute Konditionierung\\
	\\
	Verkettung $h = g(f(x))$ \quad $\kappa^h_{\ir abs}(x) = \kappa^g_{\ir abs}(f(x))\kappa^f_{\ir abs}(x)$\\
	\begin{equation*}
		\cond(\ma{A}) = \norm{\ma{A}^{-1}} \cdot \norm{\ma{A}}
	\end{equation*}
	$\cond(\ma{A}) \ra \infty$ schlecht, $\cond(\ma{A}) \ra 1$ gut

	\subsection{Fehler}
	Absolut: $\norm{\tilde f(x) - f(x)}$ \qquad\qquad Relativ: $\frac{\norm{\tilde f(x) - f(x)}}{\norm{f(x)}}$

	\subsection{Residuum}
	bezeichnet die Abweichung vom gewünschten Ergebnis, wenn Näherungslösungen eingesetzt werden.\\
	Residuum klein $\Rightarrow$ relativer Fehler $\ll 1$.
	\begin{equation*}
		\vec r = \vec b - \ma A\vec x
	\end{equation*}
\end{sectionbox}

\begin{sectionbox}
\subsection{Parametrisierung einer Geraden}
\begin{tabularx}{\columnwidth}{CCC}
\multirow{2}{*}{$g(x) = a x + b$} & $y_1 = g(x_1)$ & $a = \frac{y_1 - y_2}{x_1 - x_2}$\\
& $y_2 = g(x_2)$ & $b = \frac{x_1 y_2 - x_2 y_1}{x_1 - x_2}$
\end{tabularx}

\subsection{Schnittpunkt zweier Geraden}
\begin{tabularx}{\columnwidth}{CC}
$a_{11}x_1 + a_{12}x_2 = b_1$ & $x_1 = \frac{a_{22}b_1 - a_{12}b_2}{a_{11}a_{22}-a_{12}a_{21}}$\\
$a_{21}x_1 + a_{22}x_2 = b_2$ & $x_2 = \frac{-a_{21}b_1 + a_{11}b_2}{a_{11}a_{22}-a_{12}a_{21}}$
\end{tabularx}
\end{sectionbox}

\begin{sectionbox}
	\subsection[Matrizen]{Matrizen $\ma A \in\mathbb{K}^{m \times n}$}
	\begin{tabularx}{\columnwidth}{LX}
	$(\ma A + \ma B)^\top = \ma A^\top + \ma B^\top$ & $(\ma A \cdot \ma B)^\top = \ma B^\top \cdot \ma A^\top$\\
	${(\ma A^\top)}^{-1} = {(\ma A^{-1})}^\top$ & $(\ma A \cdot \ma B)^{-1} = \ma B^{-1}\ma A^{-1}$
	\end{tabularx}

	\subsubsection{Dimensionen}

	\begin{tablebox}{ll}
	Bildraum & Nullraum \\ \mrule
	$\Bild \ma A = \iset{\ma A \vec x}{\vec x \in \K^n }$ & $\ker\ma A = \iset{\vec x \in \K^n}{\ma A \vec x = \vec 0}$\\
	$\rang \ma A = \dim(\Bild \ma A)$ & $\defect \ma A = \dim(\ker \ma A)$\\
	\end{tablebox}
	$\rang \ma A = r$ ist Anzahl. lin. unab. Spaltenvektoren.\\
	$\ma A$ erzeugt $\mathbb K \Leftrightarrow r = n$ \qquad $\ma A$ ist Basis von $\mathbb K \Leftrightarrow r = n = m$\\
	$\dim \mathbb K = n = \rang\ma A + \dim\ker\ma A$ \qquad $\rang\ma A = \rang\ma A^\top$



	\subsubsection{Quadratische Matrizen $A \in \mathbb{K}^{n \times n}$}
	regulär/invertierbar/nicht-singulär $\Leftrightarrow \det (\ma A) \ne 0 \Leftrightarrow \rang\ma A = n$\\
	singulär/nicht-invertierbar $\Leftrightarrow \det (\ma A) = 0 \Leftrightarrow \rang\ma A \ne n$\\
	orthogonal $\Leftrightarrow \ma A^\top=\ma A^{-1} \Ra \det(\ma A) = \pm 1$\\
	symmetrisch: $\ma A=\ma A^\top$ \qquad schiefsymmetrisch: $\ma A=-\ma A^\top$
	%\item hermitsch: $\ma A=\overline{\ma A}^\top$, unitär:$\ma A^{-1} = \overline{\ma A}^\top$
	

	\subsubsection[Determinante]{Determinante von $\ma A\in \mathbb K^{n\times n}$: $\det(\ma A)=|\ma A|$}
	$\det\mat{ \ma A & \ma 0 \\ \ma C& \ma D }= \det\mat{ \ma A & \ma B \\ \ma 0 & \ma D } = \det(\ma A)\det(\ma D)$ \\
	\begin{tabular*}{\columnwidth}{@{\extracolsep\fill}ll}
	$\det(\ma A) = \det(\ma A^T)$ & $\det(\ma A^{-1}) = \det(\ma A)^{-1}$
	\end{tabular*}
	$\det(\ma A\ma B) = \det(\ma A)\det(\ma B) = \det(\ma B)\det(\ma A) = \det(\ma B\ma A)$\\
	Hat $\ma A$ 2 linear abhäng. Zeilen/Spalten $\Rightarrow |\ma A|=0$ \\
	Entwicklung. n. $j$ter Zeile: $|\ma A|=\sum\limits_{i=1}^n (-1)^{i+j} \cdot a_{ij} \cdot |\ma A_{ij}|$\\

	\subsubsection{Eigenwerte $\lambda$ und Eigenvektoren $\underline v$}
	\framebox[\columnwidth]{\large $\ma A \vec v = \lambda \vec v$ \qquad $\det \ma A = \prod \lambda_i$ \qquad $\Sp \ma A = \sum a_{ii} = \sum \lambda_i$}\\
	Eigenwerte: $\det(\ma A - \lambda \ma 1) = 0$ Eigenvektoren: $\ker(\ma A - \lambda_i \ma 1) = \vec v_i$\\
	EW von Dreieck/Diagonal Matrizen sind die Elem. der Hauptdiagonale.


	\subsubsection{Spezialfall $2 \times 2$ Matrix $A$}
	\parbox{3cm}{ $\det(\ma A) = ad-bc$ \\ $\Sp(\ma A) = a+d$ } $\mat{a & b\\ c & d}^{-1} = \frac{1}{\det \ma A} \mat{d & -b\\ -c& a}$\\
	$\lambda_{1/2} = \frac{\Sp \ma A}{2} \pm \sqrt{ \left( \frac{\mathrm{sp} \ma A}{2} \right)^2 - \det \ma A }$


	\subsubsection{Spezielle Matrizen}
	Diagonalmatrix $\ma D$:  $\det \ma D = \prod d_{i}$\\
	$\ma D^{-1} =\operatorname{diag} \left(d_1, \dots, d_n\right)^{-1} = \operatorname{diag} \left(d_1^{-1}, \dots, d_n^{-1}\right)$\\
\end{sectionbox}



\begin{sectionbox}
	\subsection{Norm $|| \cdot ||$}
	Definition: Zahl, die die „Größe“ eines Objekts $\mathcal X$ beschreibt.\\
	Jede Norm muss folgende 3 Axiome erfüllen::
	\begin{enumerate}
		\item Definitheit: $\norm{\mathcal X} \ge 0$ mit $\norm{\mathcal X} = 0 \Leftrightarrow \mathcal X = 0$
		\item absolute Homogenität:	$\norm{\alpha\cdot \mathcal X} = |\alpha| \cdot \norm{\mathcal X}$ \qquad ($\alpha$ ist skalar)
		\item Dreiecksungleichung: $\norm{\mathcal X + \mathcal Y} \leq \norm{\mathcal X} + \norm{\mathcal Y}$
	\end{enumerate}

		\subsubsection[Vektornormen]{Vektornormen: ($\vec x \in \K^n, \sum$ von $i=0$ bis $n$)}
		\begin{tablebox}{l@{\ }ll@{\ }l}
			Summen & $\norm{\vec x}_1 = \sum |x_i|$ & Euklidische & $\norm{\vec x}_2 = \sqrt{\sum |x_i|^2}$\\
			Maximum & $\norm{\vec x}_\infty = \max |x_i|$ & Alg. p-Norm & $\norm{\vec x}_p = \left( \sum |x_i|^p \right)^{1\!/\!p}$\\		
		\end{tablebox}


	\subsubsection[Matrixnormen]{Matrixnormen ($\ma A \in \K^{m \times n}, i\in[0,m], j\in[0,n]$)}
	Für Matrixnormen gilt zu den 3 Standard Axiomen zusätzlich:
	\begin{enumerate} \setcounter{enumi}{3}
		\item Submultiplikativität: $\norm{\ma A + \ma B} \leq \norm{\ma A} \cdot \norm{\ma B}$
	\end{enumerate}

	\begin{tablebox}{lr@{ = }l}
	Gesamtnorm $\left(\norm{\ma{A}}_M = \frac{\norm{\ma{A}}_G}{\sqrt{mn}}\right)$ & $\norm{\ma{A}}_G$ & $\sqrt{mn}\cdot\underset{i,j}{\max}\abs{a_{ij}}$\\
	Zeilennorm (max Zeilensumme) & $\norm{\ma{A}}_\infty$ & $\underset{i}{\max}\sum\limits_{j=1}^n\abs{a_{ij}}$ \\
	Spaltennorm (max Spaltensumme) & $\norm{\ma{A}}_1$ & $\underset{j}{\max}\sum\limits_{i=1}^n\abs{a_{ij}}$ \\
	$\underset{\text{euklidische Norm}}{\text{Frobeniusnorm}}$ $(||\ma{I}||_E = \sqrt{n})$ & $\norm{\ma{A}}_E$ & $\sqrt{\sum\limits_{i=1}\sum\limits_{j=1}\abs{a_{ij}}^2}$\\
	Spektralnorm, Hilbertnorm & $\norm{\ma{A}}_\lambda$ & $\sqrt{\lambda_\text{max}(\ma{A}^\top\cdot\ma{A})}$\\
	\end{tablebox}
\end{sectionbox}





\begin{sectionbox}
\subsection{Jacobi-Matrix}
$\vec f: \mathbb{R}^n \rightarrow \mathbb{R}^m$
\begin{equation*}
	\frac{\partial }{\partial \vec x} \vec f(\vec x) = \ma J_f (x) =
	\begin{bmatrix}
		\frac{\partial f_1}{\partial x_1} & \hdots & \frac{\partial f_1}{\partial x_n} \\
		\vdots & & \vdots \\
		\frac{\partial f_m}{\partial x_1} & \hdots & \frac{\partial f_m}{\partial x_n}
	\end{bmatrix} =
	\begin{bmatrix}
		(\nabla f_1)^T \\
		\vdots \\
		(\nabla f_m)^T
	\end{bmatrix}
\end{equation*}
\end{sectionbox}

\begin{sectionbox}
\subsection{Wichtige Formeln}
\setlength{\tabcolsep}{0pt}
\begin{tablebox}{@{\extracolsep\fill}ll@{}} 
Dreiecksungleichung: &$\big|\! \abs{x}- \abs{y}\!\big| \le \abs{x \pm y} \le \abs{x} + \abs{y}$\\
Cauchy-Schwarz-Ungleichung: & $\left| \vec x^\top \bdot \vec y \right| \le \| \vec x\| \cdot \| \vec y\|$ \\
Bernoulli-Ungleichung: & $(1+x)^n \ge 1+nx$\\ \cmrule
Arithmetische Summenformel &  $\sum \limits_{k=1}^{n} k = \frac{n (n+1)}{2} $ \\
Geometrische Summenformel &  $ \sum \limits_{k=0}^{n} q^k = \frac{1 - q^{n+1}}{1-q}$ \\
Binomialkoeffizient & $\binom nk = \binom n{n-k} = \frac{n!}{k! \cdot (n-k)!}$\\
\end{tablebox}
\end{sectionbox}

\begin{sectionbox}
\subsection{Sinus, Cosinus \quad $\sin^2(x) \bs + \cos^2(x) = 1$}
\qquad $e^{\j x} = \cos (x) + \j \cdot \sin(x)$
\setlength{\tabcolsep}{4pt}
\begin{tablebox}{@{\extracolsep\fill}c|c|c|c|c||c|c|c|c@{}} 
$x$ & $0$ & $\pi / 6$ & $\pi / 4$ & $\pi / 3$ & $\frac{1}{2}\pi$ & $\pi$ & $\frac{3}{2}\pi$ & $2 \pi$ \\
$\varphi$ & $\SI{0}{\degree}$ & $\SI{30}{\degree}$ & $\SI{45}{\degree}$ & $\SI{60}{\degree}$ & $\SI{90}{\degree}$ & $\SI{180}{\degree}$ & $\SI{270}{\degree}$ & $\SI{360}{\degree}$ \\ \cmrule
$\sin$ & $0$ & $\frac{1}{2}$ & $\frac{1}{\sqrt{2}}$ & $\frac{\sqrt 3}{2}$ & $1$ & $0$ & $-1$ & $0$ \\
$\cos$ & $1$ & $\frac{\sqrt 3}{2}$ & $\frac{1}{\sqrt 2}$ & $\frac{1}{2}$ & $0$ & $-1$ & $0$ & $1$ \\
$\tan$ & $0$ & $\frac{\sqrt{3}}{3}$ &	$1$	&	$\sqrt{3}$ & $\pm \infty$ & $0$ & $\mp \infty$ & $0$\\ 
\end{tablebox}
\begin{tabular*}{\columnwidth}{@{\extracolsep\fill}ll@{}}
	Additionstheoreme &  Stammfunktionen\\
 	$\cos (x - \frac{\pi}{2}) = \sin x$ & $\int x \cos(x) \diff x = \cos(x) + x \sin(x)$\\
 	$\sin (x + \frac{\pi}{2}) = \cos x$ & $\int x \sin(x) \diff x = \sin(x) - x \cos(x)$\\
 	$\sin 2x = 2 \sin x \cos x $  & $\int \sin^2(x) \diff x = \frac12 \bigl(x - \sin(x)\cos(x) \bigr)$\\
 	$\cos 2x = 2\cos^2 x - 1$  & $\int \cos^2(x) \diff x = \frac12 \bigl(x + \sin(x)\cos(x) \bigr)$\\
 	$\sin(x) = \tan(x)\cos(x)$ & $\int \cos(x)\sin(x) = -\frac12 \cos^2(x)$ \\
\end{tabular*}\\[1em]
	\textbf{Sinus/Cosinus Hyperbolicus} $\sinh, \cosh$\\
	\begin{tabular*}{\columnwidth}{@{\extracolsep\fill}ll@{}}
	$\sinh x = \frac{1}{2}(e^x -e^{-x})= - \j \, \sin(\j x)$ & $\cosh^2 x  \bs - \sinh^2 x = 1$\\
	$\cosh x  = \frac{1}{2}(e^x +e^{-x})= \cos(\j x)$ & $\cosh x + \sinh x = e^{x}$\\
	\end{tabular*}\\
	\textbf{Kardinalsinus} $\mathrm{si}(x) = \frac{\sin(x)}{x}$ \qquad \textbf{genormt:} $\sinc(x) = \frac{\sin(\pi x)}{\pi x}$
\end{sectionbox}

\begin{sectionbox}
	\subsection{Integrale $\int e^x\;\mathrm dx = e^x = (e^x)'$}
	\begin{tabularx}{\columnwidth}{lX}
	Partielle Integration: & $\int uw'=uw-\int u'w$\\
	Substitution: & $\int f(g(x)) g'(x)\diff x=\int f(t)\diff t$
	\end{tabularx}
	\begin{tablebox}{@{\hspace{5mm}}c@{\extracolsep\fill}c@{\extracolsep\fill}c@{\hspace{5mm}}} 
	\renewcommand{\arraystretch}{1.6}
		$F(x)$ & $f(x)$ & $f'(x)$ \\ \cmrule
		$\frac{1}{q+1}x^{q+1}$ & $x^q$ & $qx^{q-1}$ \\
		\raisebox{-0.2em}{$\frac{2\sqrt{ax^3}}{3}$} & $\sqrt{ax}$ & \raisebox{0.2em}{$\frac{a}{2\sqrt{ax}}$}\\
		$x\ln(ax) -x$ & $\ln(ax)$ & $\textstyle \frac{1}{x}$\\
		$\frac{1}{a^2} e^{ax}(ax- 1)$ & $x \cdot e^{ax}$ & $e^{ax}(ax+1)$ \\
		$\frac{a^x}{\ln(a)}$ & $a^x$ & $a^x \ln(a)$ \\
		$-\cos(x)$ & $\sin(x)$ & $\cos(x)$\\
		$\cosh(x)$ & $\sinh(x)$ & $\cosh(x)$\\
		$-\ln |\cos(x)|$ & $\tan(x)$ & $\frac{1}{\cos^2(x)}$ \\ 
	\end{tablebox}

	\begin{tabularx}{\columnwidth}{Ll@{}}
	\multicolumn{2}{c}{$\int e^{at} \sin(bt) \diff t = e^{at} \frac{a \sin(bt) + b \cos(bt)}{a^2 + b^2}$}\\
	$\int \frac{\diff t}{\sqrt{at+b}} = \frac{2 \sqrt{at+b}}{a}$ & $\int t^2 e^{at} \diff t = \frac{(ax-1)^2+1}{a^3} e^{at}$\\
	$\int t e^{at} \diff t = \frac{at-1}{a^2} e^{at}$ & $\int x e^{ax^2} \diff x = \frac{1}{2a} e^{ax^2}$\\
	\end{tabularx}
\end{sectionbox}

\begin{sectionbox}
\subsection{Exponentialfunktion und Logarithmus}
\begin{tabular*}{\columnwidth}{l@{\extracolsep\fill}ll}
	$a^x = e^{x \ln a}$ & $\log_a x = \frac{\ln x}{\ln a}$ & $\ln x \le x -1$\\
	$\ln(x^{a}) = a \ln(x)$ & $\ln(\frac{x}{a}) = \ln x - \ln a$ & $\log(1) = 0$\\
\end{tabular*}

\subsection{Reihen}
\begin{tabularx}{\columnwidth}{CCC}
$\underset{\text{Harmonische Reihe}}{\sum\limits_{n=1}^\infty \frac{1}{n} \ra \infty}$ & $\underset{\text{Geometrische Reihe}}{\sum\limits_{n=0}^\infty q^n \stackrel{|q|<1}= \frac{1}{1-q}}$ & $\underset{\text{Exponentialreihe}}{\sum\limits_{n = 0}^{\infty} \frac{z^n}{n!} = e^z}$
\end{tabularx}
\end{sectionbox}

\section{Lösung nichtlinearer Gleichungen}
% =============================================================================================================00
Exakte Lösung $x*$, Fehler $\epsilon=x-x*$


\begin{sectionbox}
	\subsection{Iterationsverfahren (Nullstellensuche)}
	Problem: $f(x) = 0, f(x)$ stetig in $[a,b]$ und $f(a) \cdot f(b) < 0$\\
	Gesucht: $x^*:f(x^*)=0, a \le x^* \le b$

	Konvergenz: $ε^{(k+1)} = \frac{1}{2} ε^{(k)} = \left( \frac12 \right)^{k+1} ε^{(0)} $\\
	Iterationsschritte bis $ε < τ$: $k = \ceil{\ld\left(\frac{ε^{(0)}}{τ}\right)}$
\end{sectionbox}

\begin{sectionbox}
\subsection{Fixpunktiteration}
NST-Problem als FP-Problem: $x = F(x) + x$ 
\begin{equation*}
	x^{(k + 1)} = \Phi(x^{(k)})
\end{equation*}
\begin{equation*}
	x^\star = \Phi(x^\star)
\end{equation*}
Falls $\abs{\Phi'(x^\star)} < 1 \Rightarrow$ Konvergenz bzw. stabiler Fixpunkt \\
Falls $0 < \abs{\Phi'(x^\star)} < 1 \Rightarrow$ lineare Konvergenz mit
\begin{equation*}
	\varepsilon^{(k + 1)} \approx \Phi'(x^\star) \varepsilon^{(k)}
\end{equation*}
Falls $\Phi'(x^\star) = 0$ und $\Phi''(x^\star) \ne 0 \Rightarrow$ quadratische Konvergenz \\
\textbf{Allgemein:} Konvergenzordnung $n \Leftrightarrow$\\ $\Phi'(x^\star) = \Phi''(x^\star) = \ldots = \Phi^{(n - 1)}(x^\star) = 0$ und $\Phi^{(n)}(x^\star) \ne 0$
\end{sectionbox}

\begin{sectionbox}
\subsection{Newton-Raphson}
	Funktion durch Gerade annähern und Nullstelle bestimmen. An dieser Stelle den Vorgang wiederholen. Nur lokale Konvergenz\\
\textbf{Ausgangsproblem:}
\begin{equation*}
	f(x) = 0
\end{equation*}
\begin{equation*}
	x^{(k + 1)} = x^{(k)} - \frac{f(x^{(k)})}{f'(x^{(k)})}  =: Φ\left(x^{(k)}\right)
\end{equation*}

\subsubsection{Konvergenz}
Falls $f'(x^\star) \ne 0$ (einfache Nullstelle) $\Rightarrow$ quadratische Konvergenz mit
\begin{equation*}
	\varepsilon^{(k + 1)} = \frac{1}{2} \frac{f''(x^\star)}{f'(x^\star)} {\varepsilon^{(k)}}^2
\end{equation*}
Falls $f'(x^\star) = 0$ (Nullstellengrad $n > 1$) $\Rightarrow$ lineare Konvergenz mit
\begin{equation*}
\textbf{Konvergenzfaktor} = \frac{n-1}{n}
\end{equation*}

\subsubsection{Sekanten-Methode}
Falls die Auswertung von $f'(x)$ vermieden werden soll:
\begin{equation*}
	x^{(k + 1)} = x^{(k)} - \frac{f(x^{(k)}) \left( x^{(k)} - x^{(k - 1)} \right)}{f(x^{(k)}) - f(x^{(k - 1)})}
\end{equation*}

\subsubsection{Mehrdimensional}
\textbf{Theoretisch:}
\begin{equation*}
	\vec x^{(k + 1)} = \vec x^{(k)} - \ma J^{-1}_{\vec f}\left (\vec x^{(k)}\right ) \vec f(x^{(k)})
\end{equation*}
\textbf{Praktisch:}
\begin{equation*}
	\ma J_{\vec f}\left (\vec x^{(k)}\right ) \vec x^{(k + 1)} = \ma J_{\vec f}\left (\vec x^{(k)}\right )\cdot \left (\vec x^{(k)} - \vec f(x^{(k)}\right )
\end{equation*}
\end{sectionbox}






\section{Lösung linearer Gleichungssysteme}
% ==========================================================================================================
\begin{sectionbox}
\textbf{Ausgangsproblem:}
\begin{equation*}
	\ma{A} \vec{x} = \vec{b}
\end{equation*}
\begin{tablebox}{ll}
		$\ma{A} = \ma{M} - \ma{N}$ & Systemmatrix\\
		$\ma{D}$ & Diagonalmatrix \texttt{diag(diag(}$\ma{A}$\texttt{))}\\
		$\ma{L}$ & Linke untere Dreiecksmatrix \texttt{tril(}$\ma{A},-1$\texttt{)}\\
		$\ma{U}$ & Rechte obere Dreiecksmatrix \texttt{triu(}$\ma{A},1$\texttt{)}\\
\end{tablebox}
$\ma A = \ma D + \ma L + \ma U$, keine LR-Zerlegung!
\end{sectionbox}

\begin{sectionbox}
\subsection{Jacobi-Verfahren}
Konvergiert falls $\rho(\ma{K}_j) < 1$ oder falls $\ma{A}$ strikt diagonaldominant\\
Spektralradius $\rho(\ma{K}_j) = \max |\lambda_i(\ma A)|$ mit $\lambda_i$ EW.
\begin{equation*}
	\ma{K}_j = \ma{D}^{-1} (- \ma{L} - \ma{U})
\end{equation*}
\emph{Matrixdarstellung:}
\begin{equation*}
	\vec{x}^{(k + 1)} = \ma{K}_j \vec{x}^{(k)} + \ma{D}^{-1} \vec{b}
\end{equation*}
\emph{Komponentenweise:}
\begin{equation*}
	x_i^{(k + 1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j = 1, j \ne i}^n a_{ij} x_{j}^{(k)} \right)
\end{equation*}
\textbf{Vorkonditionierung:}
\begin{equation*}
	\ma P = \ma D^{-1}\quad\Rightarrow\quad\ma P\ma A \vec x = \ma P\vec b
\end{equation*}
\end{sectionbox}

\begin{sectionbox}
\subsection{Gauß-Seidel-Verfahren}
	Unterschied zu Jacobi: Komponentenweise Berechnung von $\vec x$ mit bereits iterierten Werten. (Kürzere Iterationszyklen)
\begin{equation*}
	\ma{K}_{gs} = - (\ma{D} + \ma{L})^{-1} \ma{U}
\end{equation*}
\emph{Matrixdarstellung:}
\begin{equation*}
	\vec{x}^{(k + 1)} = \ma{K}_{gs} \vec{x}^{(k)} + (\ma{D} + \ma{L})^{-1} \vec{b}
\end{equation*}
\emph{Komponentenweise:}
\begin{equation*}
	x_i^{(k + 1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j = 1}^{i - 1} a_{ij} x_{j}^{(k + 1)} - \sum_{j = i + 1}^{n} a_{ij} x_{j}^{(k)}\right)
\end{equation*}
Konvergiert falls $\rho(\ma{K}_{gs}) < 1$ oder $\ma{A}$ strikt diagonaldominant oder $\ma{A}$ positiv definit \\
Falls $A$ tridiagonal und positiv definit
\begin{equation*}
	\rho(\ma{K}_{gs}) = \rho(\ma{K}_j)^2
\end{equation*}
\textbf{Vorkonditionierung:}
\begin{equation*}
	\ma P = (\ma D + \ma L)^{-1}\quad\Rightarrow\quad\ma P\ma A \vec x = \ma P\vec b
\end{equation*}
\end{sectionbox}

\begin{sectionbox}
\subsection{Successive Over-Relaxation}
\begin{equation*}
	\ma{K}_{SOR} = (\ma{D} + \omega \ma{L})^{-1} (\ma{D}(1 - \omega) - \omega \ma{U})
\end{equation*}
\emph{Matrixdarstellung:}
\begin{equation*}
	\vec{x}^{(k + 1)} = \ma{K}_{SOR} \vec{x}^{(k)} + (\ma{D} + \omega \ma{L})^{-1} \omega \vec{b}
\end{equation*}
	\emph{Komponentenweise:}\\
$x_i^{(k+1)} = \omega a_{ii}^{-1} \left( \vec b_i - \sum\limits_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum\limits_{j = i +1}^{n} a_{ij} x_j^{(k)} \right) + (1 - \omega) x_i^{(k)}$\\
Optimale Konvergenz für
\begin{equation*}
	\omega_\text{opt} = \argmin_{\omega} \rho(\ma{K}_{SOR})
\end{equation*}
Falls $A$ positiv definit und tridiagonal $\Rightarrow \rho(\ma{K}_{gs}) = \rho(\ma{K}_j)^2 < 1$:
\begin{equation*}
	\omega_\text{opt} = \frac{2}{1 + \sqrt{1 - \rho(\ma{K}_j)^2}}
\end{equation*}
\end{sectionbox}

\begin{sectionbox}
\subsection{Gradienten-Verfahren}
\textbf{Voraussetzungen:} $\ma A = \ma A^T$ und $\ma A$ positiv definit
\begin{equation*}
	\Phi(\vec{x}) = \frac{1}{2} \vec{x}^T \ma{A} \vec{x} - \vec{x}^T \vec{b}
\end{equation*}
\begin{equation*}
	\vec{r}^{(k)} = \vec{b} - \ma{A} \vec{x}^{(k)}
\end{equation*}
\begin{equation*}
	\alpha^{(k)} = \frac{\vec{r}^{(k)T} \vec{r}^{(k)}}{\vec{r}^{(k)T} \ma{A} \vec{r}^{(k)}}
\end{equation*}
Optimierung: $\vec{r}^{(k+1)} = \vec{r}^{(k)} - \alpha^{(k)} \ma A \vec{r}^{(k)}$
\emph{Matrixdarstellung:}
\begin{equation*}
	\vec{x}^{(k + 1)} = \vec{x}^{(k)} + \alpha^{(k)} \vec{r}^{(k)}
\end{equation*}
\end{sectionbox}


% ======================================================================================
\section{Matrix Zerlegung}
% ======================================================================================
\begin{sectionbox}
	\subsection{LR-Zerlegung von Matrizen (\textbf{L}ower and \textbf{U}pper)}
Geeignetes Lösungsverfahren für $\ma A \vec x = \vec b$, falls $n < 500$\\
$\ma A = \ma L \cdot \ma R$ \quad mit $\ma R$ obere Dreiecksmatrix ($\rang \ma A = \rang\ma R$)\\

	\subsubsection{Pivotisierung (Spaltenpivotsuche)}
	Permutationsmatrix $\ma P^\top = \ma P^{-1}$ vertauscht Zeilen, damit LR Zerlegung bei 0 Einträgen möglich ist.
	Tausche so, dass man durch die betragsmäßig größte Zahl dividiert (Pivotelement) %Verhindert Auslöschung

	\subsubsection{Rechenaufwand (FLOPS)}
	\begin{tablebox}{ll}
			LU-Zerlegung & $\frac{2}{3}n^3 - \frac{1}{2}n^2 - \frac{1}{6}n$\\
			Vorwärtseinsetzen & $n^2 - n$\\
			Rückwärtseinsetzen & $n^2$\\
	\end{tablebox}
\end{sectionbox}

\begin{sectionbox}
\begin{cookbox}{LR-Zerlegung mit Gaußverfahren}
	\item Zerlegen des Problems $\ma A \vec x = \vec b$ in das Problem $\ma L (\ma R \vec x) = \vec b$  mit $\ma A = \ma L \ma R$ bzw. $\ma L \vec y = \ma P \vec b$ (mit Pivotisierung)
	\item Zerlegungsmatrix (für $2 \times 2$): \\ $\ma A = \mat{a & b \\ c & d} \ra \mat{a & b \\ \frac{c}{a} & d - \frac{c}{a} b} = \ma A^*$ mit den Eliminationsfaktoren $l_{ik} = \frac{a_{ik}}{a_{kk}} \overset{z.B.}{=} \frac{c}{a}$
	\item Für jede Spalte der unteren Dreiecksmatrix wiederholen.\\
		 Für eine $3 \times 3$ Matrix bräuchte man 2 Durchläufe, da 3 Spalten Elimationsfaktoren bestimmt werden müssen.
	\item $\ma R = \texttt{triu}(\ma A^*)$\\
	 (obere Dreiecksmatrix von $\ma A^*$, inkl. Diagonalelemente)
	\item $\ma L = \texttt{tril}(\ma A^*,-1)+\ma 1$\\
	 (untere Dreiecksmatrix mit $1$en auf der Diagonale.
	\item \textbf{Vorwärtseinsetzen:} $\ma L \vec y = \vec b$ bzw. $\ma L \vec y = \ma P \vec b$ (mit Pivotisierung)\quad (Löse nach $\vec y$)
	\item \textbf{Rückwärtseinsetzen:} $\ma R \vec x = \vec y$ \quad (Löse nach $\vec x$)

\end{cookbox}
\end{sectionbox}


\begin{sectionbox}
		\subsection{QR-Zerlegung (existiert immer)}
	$\ma A = \ma Q \ma R$ mit $\ma Q^{-1} = \ma Q^\top$\\
	Berechnung (Verfahren): Housholder (numerisch stabil) , Gram-Schmidt, Givens Rotation.\\
	$\ma A \xrightarrow{EZF} \ma H\ma A \xrightarrow{EZF} \tilde{\ma H} \ma H \ma A = \ma R \Ra \ma A = \ma H^\top \tilde{\ma H}^\top \ma R$\\
	Aufgabe: Finde Vektor $\vec v$ der Senkrecht auf $\ma H$ steht.\\

	\subsubsection*{Lösen von LGSen mit der $Q R$ Zerlegung}
	Bestimme $\vec x$ durch Rückwärtssubsitution aus $\ma R \vec x = \ma Q^\top \vec b$
\end{sectionbox}

\begin{sectionbox}
	\begin{cookbox}{QR-Zerlegung mit Householder-Transformation für $\ma A \in \mathbb R^{m\times n }$}
		\item Setze $\vec a = \vec s_1$ (erste Spalte) und $\vec v = \vec a + \sgn ( a_1) \norm{\vec a} \vec e_1$
		\item Berechne \emph{Householder}-Trafomatrix $\ma H_{\vec v_1} = \ma 1_m - \frac{2}{\vec v^\top \vec v} \vec v \vec v^\top$
		\item Erhalte $\ma A^* = \ma H_{\vec v_1} \ma A$ (ersten Spalte bis auf $a_{11}$ nur Nullen)
		\item Wiederhole für $\ma A^*$ ohne 1. Zeile und Spalte (Untermatr. $\ma A^*_{11}$)\\
			Erweitere $\ma H^*_{\vec v_2}$ oben mit $\ma 1_m$ zu $\ma H_{\vec v_2}$ ($h_{11} = 1$)
		\item Nach $p = \min \eset{m - 1, n}$ Schritten: $\ma H_{\vec v_p} \ma A^* = \ma R$ weil
		\item $\ma Q^\top = \ma H_{\vec v_p} \cdots \ma H_{\vec v_1}$ und  $\ma Q^\top \ma A = \ma R$
	\end{cookbox}
\end{sectionbox}


\begin{sectionbox}
	\subsection{Orthogonalisierungsverfahren nach Gram-Schmidt}
	Berechnet zu $n$ Vektoren $\vec v_i$ ein Orthogonalsystem $\vec b_i$\quad ($i \in [1;n])$
	\begin{equation*}
		\vec b_1 = \vec v_1 \qquad\qquad \vec b_i = \vec v_i - \sum\limits_{k=1}^{i-1} \frac{\vec b_k^\top \bdot \vec v_i}{\vec b_k^\top \bdot \vec b_k} \vec b_k
	\end{equation*}
	Erhalte Ortho\textbf{normal}system durch $\vec b_i' = \vec b_i/\norm{\vec b_i}$\\
	QR-Zerlegung: $\ma A = \ma Q \ma R$ mit $\ma Q = \big[\vec b_1', ... , \vec b_n'\big]$ \quad $\ma R = \ma Q^\top \ma A$
\end{sectionbox}



\begin{sectionbox}
	\subsection[Givens Rotation (Jacobi-Rotation)]{Givens Rotation (Jacobi-Rotation) \quad $\ma G^{-1} = \ma G^\top$}
	Die orthogonale Givens-Rotationsmatrix $\ma G$ entspricht der Einheitsmatrix wobei 4 Elemente die Form $\mat{c & s \\ -s & c}$ haben. Die $c$ beliebig auf der Hauptdiagonalen und $s/\!-\!s$ in der gleichen Zeile/Spalte wie die $c$.

	\begin{cookbox}{QR-Zerlegung mit Givens-Rotation für $\ma A \in \mathbb R^{m\times n }$}
		\item Initialisierung: Setze $\ma R = \ma A$ und $\ma G_\text{gesamt} = \ma 1_m$
		\item Wiederhole folgende Schritte für alle Elemente $r_{xy}$ in $\ma R$, welche $0$ werden müssen um obere Dreiecksmatrix zu erhalten. (Reihe $x$, Spalte $y$), verfahre spaltenweise (links nach rechts) und in jeder Spalte von oben nach unten:
		\item Setze $a = r_{yy}$ (Hauptdiagonalelement in dieser Spalte)
		\item Setze $b = r_{xy}$ (Wert, welcher durch $0$ ersetzt werden soll)
		\item Berechne $c := \frac{a}{p}$ und $s := \frac{b}{p}$ mit $p := \sqrt{a^2 + b^2}$
		\item Setze $\ma G = (g_{ij}) =
						\begin{cases}
							c & i = x, j = x \\
							c & i = y, j = y \\
							s & i = y, j = x \\
							-s & i = x, j = y \\
							\text{Einheitsmatrix} & \text{sonst}
						\end{cases}$
		\item Setze $\ma R = \ma G \ma R$ und $\ma G_\text{gesamt} = \ma G \ma G_\text{gesamt}$
		\item Fahre, falls nötig, mit nächstem Element in $\ma R$ fort
		\item Erhalte $\ma Q = \ma G_\text{gesamt}^\top$ \quad Löse 
	\end{cookbox}
\end{sectionbox}

\begin{sectionbox}
\subsection{Dünnbesetzte Matrizen}
\parbox{3cm}{\textbf{Ziel:} effizienteres Speichern von Matrizen mit vielen 0 Einträgen.} \qquad
$
\ma A = \begin{bmatrix}
a & 0 & 0 & 0\\
0 & b & c & 0\\
0 & 0 & 0 & d\\
e & 0 & f & 0
\end{bmatrix}
$

\begin{tablebox}{@{}lccc}
				& \textbf{COO} 		& \textbf{CRS} 		& \textbf{CCS}		\\
\cmrule
\texttt{row} 	& $\{1,2,2,3,4,4\}$ & 					& $\{1,4,2,2,4,3\}$	\\
\texttt{rowptr} & 					& $\{1,2,4,5,7\}$ & 					\\
\texttt{col} 	& $\{1,2,3,4,1,3\}$ & $\{1,2,3,4,1,3\}$ & 					\\
\texttt{colptr}	& 					& 					& $\{1,3,5,6,7\}$	\\
\texttt{val} 	& $\{a,b,c,d,e,f\}$ & $\{a,b,c,d,e,f\}$ & $\{a,b,c,d,e,f\}$	\\
\end{tablebox}
\begin{tabularx}{\columnwidth}{@{}lX}
\textbf{COO} & Zeilen und Spaltenindex von \texttt{val}\\
\textbf{CRS} & \texttt{rowptr}(i) zeigt auf j-tes Element von \texttt{col}\\
\textbf{CCS} & \texttt{colptr}(i) zeigt auf j-tes Element von \texttt{row}\\
\end{tabularx}
\end{sectionbox}


% ======================================================================================
\section{Numerische Differentiation}
% ======================================================================================
\begin{sectionbox}
\subsection{Vorwärtsdifferenz}
\begin{equation*}
	f'(x_0) \approx \tilde{f}_\text{Vor}'(x_0) = \frac{f(x_0 + h) - f(x_0)}{h}
\end{equation*}
\begin{equation*}
	f'(x_0) - \tilde{f}_\text{Vor}'(x_0) \in \mathcal{O}(h)
\end{equation*}

\subsection{Rückwärtsdifferenz}
\begin{equation*}
	f'(x_0) \approx \tilde{f}_\text{Rück}'(x_0) = \frac{f(x_0) - f(x_0 - h)}{h}
\end{equation*}
\begin{equation*}
	f'(x_0) - \tilde{f}_\text{Rück}'(x_0) \in \mathcal{O}(h)
\end{equation*}

\subsection{Zentrale Differenz}
\begin{equation*}
	f'(x_0) \approx \tilde{f}_\text{Zentral}'(x_0) = \frac{f(x_0 + h) - f(x_0 - h)}{2h}
\end{equation*}
\begin{equation*}
	f'(x_0) - \tilde{f}_\text{Zentral}'(x_0) \in \mathcal{O}(h^2)
\end{equation*}
	$h_{opt} = \sqrt[3]{\frac{3\epsilon}{M}}$ \qquad Max. Rundungsfehler $\epsilon$
\end{sectionbox}


% ======================================================================================
\section{Numerische Integration}
% ======================================================================================
\begin{sectionbox}
\subsection{Polynom-Ansätze}
\begin{equation*}
	\int_a^b f(x) \diff x \approx \int_a^b P(x) \diff x
\end{equation*}

\subsubsection{Lagrange}
\begin{equation*}
	P(x) = \sum_{k = 0}^n L_{n, k}(x)\cdot f(x_k)
\end{equation*}
\begin{equation*}
	L_{n, k}(x) = \prod_{i = 0, i \ne k}^n \frac{x - x_i}{x_k - x_i}
\end{equation*}

\subsubsection{Differenzen}
\begin{equation*}
	f[x_i, \dots, x_j] = \frac{f[x_{i + 1}, \dots, x_j] - f[x_i, \dots x_{j - 1}]}{x_j - x_i}
\end{equation*}
\begin{equation*}
	P(x) = f[x_0] + \sum_{k = 1}^n f[x_0, \dots, x_k] (x - x_0) \dots (x - x_{k - 1})
\end{equation*}
\end{sectionbox}

\begin{sectionbox}
\subsection{Newton-Cotes}
\begin{equation*}
	\int_a^b f(x) \diff x \approx \sum_{i = 0}^n g_i f(x_i)
\end{equation*}
\begin{equation*}
	h = \frac{b - a}{n}
\end{equation*}

\subsubsection{Trapez}
falls $n = 1$:
\begin{equation*}
	\int_a^b f(x) \diff x \approx (b - a) \frac{f(a) + f(b)}{2}
\end{equation*}
\textbf{Allgemein:}
\begin{equation*}
	\int_a^b f(x) \diff x \approx h \left( \frac{f(a) + f(b)}{2} + \sum_{k = 1}^{n - 1} f(a + k \cdot h) \right)
\end{equation*}
\end{sectionbox}

\begin{sectionbox}
\subsubsection{Simpson $\frac{1}{3}$ (Fassregel)}
falls $n = 2$:
\begin{equation*}
	\int_a^b f(x) \diff x \approx \frac{b - a}{6} (f(a) + 4 f(a + h) + f(b))
\end{equation*}
\textbf{Allgemein} (zusammengesetzte Simpsonregel):
\begin{equation*}
	\int_a^b f(x) \diff x \approx \frac{h}{3} \left( f(a) + f(b) + \sum_{k = 1}^{n - 1} a_k f(a + k \cdot h) \right)
\end{equation*}
\begin{equation*}
	a_k = 3 + (-1)^{k + 1}
\end{equation*}

\subsubsection{Simpson $\frac{3}{8}$}
falls $n = 3$:
\begin{equation*}
	\int_a^b f(x) \diff x \approx \frac{3h}{8} (f(a) + 3 f(a + h) + 3 f(a + 2h) + f(b))
\end{equation*}
\end{sectionbox}

\begin{sectionbox}
\subsection{Kubische Splines $S(x)$}
Stückweise Approximation von $f(x)$ durch $n$ kubische Polynome mit $S(x_i) = f(x_i)$\\
Bestimme Parameter $a,b,c,d$ für jedes Teilstück:
\begin{align*}
		S_j(x) &= a_j + b_j (x-x_j) + c_j (x-x_j)^2 + d_j (x-x_j)^3\\
		S_j'(x) &= b_j+ 2c_j (x-x_j) + 3d_j (x-x_j)^2
\end{align*}
Für $j=0,1,\ldots,n-1$:
\begin{equation*}
	S_j(x_j) = f(x_j) \wedge S_j(x_{j+1}) = f(x_{j+1})
\end{equation*}
Für $j=0,1,\ldots,n-2$:
\begin{align*}
	S_j(x_{j+1}) &\stackrel{!}{=} S_{j+1}(x_{j+1}) ≡ a_{j+1}\\
	S_j'(x_{j+1}) &\stackrel{!}{=} S_{j+1}'(x_{j+1}) ≡ b_{j+1} \\
	S_j''(x_{j+1}) &\stackrel{!}{=} S_{j+1}''(x_{j+1}) ≡ 2c_{j+1}
\end{align*}
Freier bzw. natürlicher Rand:
\begin{equation*}
	S''(x_0) = S''(x_n) = 0
\end{equation*}
Eingespannter Rand:
\begin{equation*}
	S'(x_0) = f'(x_0) \wedge S'(x_n) = f'(x_n)
\end{equation*}
\end{sectionbox}


\begin{sectionbox}
\begin{cookbox}{Parameterbestimmung}
\item $a_j = f(x_j)$ und $h_j = x_{j+1} - x_j$
\item Löse LGS für $\vec c$: $\ma A \vec c = \vec l$\\
\scalebox{.9}{$\ma A = \begin{bmatrix}
1 & 0 & 0 & \dots & 0\\
h_0 & 2(h_0+h_1) & h_1 & \ddots & \vdots\\
\ddots & \ddots & \ddots & \ddots & \vdots\\
\vdots & \ddots & h_{n-2} & 2(h_{n-2}-h_{n-1}) & h_{n-1}\\
0 & \dots & 0 & 0 & 1
\end{bmatrix}$}\\
$\vec l = \begin{bmatrix}
0 \\ \frac{3}{h_1}(a_2 - a_1) - \frac{3}{h_0}(a_1 - a_0) \\ \vdots \\ \frac{3}{h_{n-1}}(a_{n} - a_{n-1}) - \frac{3}{h_{n-2}}(a_{n-1} - a_{n-2}) \\ 0
\end{bmatrix}$
\item $b_j = \frac{1}{h_j}(a_{j+1}- a_j)-\frac{h_j}{3}(2c_j+c_{j+1})$
\item $d_j = \frac{1}{3h_j}(c_{j+1}-c_j)$
\end{cookbox}
\end{sectionbox}

\vfill

% ======================================================================================
\section{Least Squares}
% ======================================================================================
\begin{sectionbox}
	\subsection{Ausgleichsrechnung}
	Gegeben: $n$ Datenpunkte $(x_i,y_i)$, Gesucht: Eine Polynom-Funktion $f$ welche die Datenpunkte möglichst gut (kleinstes Fehlerquadrat) approximiert.
	Es gilt: $\vec f_{\vec \alpha}(\vec x) = \vec y + \vec r \approx \vec y$ mit Residum $\vec r$\\
	Bestimme $k$ Parameter $\alpha_j$ so, dass Fehlerquadrat $\vec r^\top\vec r$ minimiert wird.\\
	Erstelle $\ma A \in \R^{n \times k}$ mit $\ma A \vec \alpha \approx \vec f(\vec x)$, Zeilen aus $k$ $x$-Termen: $x^2,x,1$\\
	$\min\limits_\alpha \vec r = \min\limits_\alpha \norm{\ma A \vec \alpha - \vec y}_2^2 = \min\limits_\alpha \norm{\vec y - \ma A \vec \alpha}_2^2$\\
	Minimierung durch Ableitung: $\forall j\in[1,k]:\frac{\partial (\vec r)^2}{\partial \alpha_j} \stackrel{!}{=} 0$\\
	Dadurch ergibt sich: $\ma A^\top \ma A \vec \alpha = \ma A^\top \vec y$
	\begin{cookbox}{Lösen der Normalengleichung}
		\item Bestimme eine reduzierte QR-Zerlegung \\ $\ma A = \tilde{\ma Q}  \tilde{\ma R}$ mit $\tilde{\ma Q} \in \mathbb R^{n \times k}, \tilde{\ma R} \in \mathbb R^{k \times k}$
		\item Löse $\tilde{\ma R } \vec x =\tilde{\ma Q}^\top \vec y$
	\end{cookbox}

	\subsubsection{Lineare Ausgleichsrechnung ($k=2$)}
	$f_{\vec \alpha}(x) = \alpha_1 x + \alpha_0$  \qquad $\ma A = [\vec x \quad \vec 1]$ \quad $\vec \alpha = \mat{α_1 \\ α_0}$
	\begin{equation*}
		\argmin_{α_1, α_0} E(α_1, α_0) = \sum_{i = 1}^n \left( y_i - (α_1 x_i + α_0) \right)^2
	\end{equation*}

	\subsubsection{Polynomial Least Squares}
	\begin{equation*}
		f_{\vec \alpha}(x) = P(x, \vec \alpha) = a_k x^k + \ldots + a_1 x + a_0
	\end{equation*}
	\begin{equation*}
		\argmin_{\alpha_0, \ldots, \alpha_k} E_k(\alpha_0, \ldots, \alpha_k) = \sum_{i = 1}^k \left( y_i - P(x_i) \right)^2
	\end{equation*}	
	Minimierung durch Ableitung: $\forall i\in[0,n]:\frac{\partial E_n}{\partial a_i} \stackrel{!}{=} 0$
\end{sectionbox}

\begin{sectionbox}
\subsection{Anwendung in der linearen Ausgleichsrechnung}
(Minimierung d. Restes)\\
Problem: $\ma A^\top \ma A \vec x = \ma A^\top \vec b$ mit $\ma A \in \mathbb R^{m \times n }$ und $\vec b \in \mathbb R^{m}$ \\
\begin{cookbox}{Lösen der Normalengleichung}
	\item Bestimme eine reduzierte QR-Zerlegung \\ $\ma A = \tilde{\ma Q}  \tilde{\ma R}$ mit $\tilde{\ma Q} \in \mathbb R^{m \times n}, \tilde{\ma R} \in \mathbb R^{n \times n}$
	\item Löse $\tilde{\ma R } \vec x =\tilde{\ma Q}^\top \vec b$
\end{cookbox}
$\norm{\vec b - \ma A \vec x}^2 = \norm{\ma Q^\top (\vec b - \ma A \vec x)}^2 = \norm{\tilde{\vec b} - \tilde{\ma R} \vec x}^2 + \norm{\vec c}^2 \ge \norm{\vec c^2}$
\end{sectionbox}

\vfill

% ======================================================================================
\section{Numerische Lösung von Differentialgleichungen}
% ======================================================================================
\begin{sectionbox}
\textbf{Ausgangsproblem:}
\begin{equation*}
	\dot{x}(t) = f(x(t))
\end{equation*}
\begin{equation*}
	\hat{x}(\nu) = x(t_0 + \Delta t \nu)
\end{equation*}
\begin{equation*}
	\hat{f}(\nu) = f(t_0 + \Delta t \nu)
\end{equation*}

\subsection{Expliziter Euler}
\begin{equation*}
	\hat{x}(\nu + 1) = \hat{x}(\nu) + \Delta t \hat{f}(\nu)
\end{equation*}
stabil für $0 < Δt < 2$, instabil für $Δt >$ 
\subsection{Impliziter Euler}
\begin{equation*}
	\hat{x}(\nu + 1) = \hat{x}(\nu) + \Delta t \hat{f}(\nu + 1)
\end{equation*}

\subsection{Trapez}
\begin{equation*}
	\hat{x}(\nu + 1) = \hat{x}(\nu) + \frac{\Delta t}{2} (\hat{f}(\nu) + \hat{f}(\nu + 1))
\end{equation*}
\end{sectionbox}

\begin{sectionbox}
\subsection{Gear}
\begin{equation*}
	\hat{x}(\nu + 2) = \frac{4}{3} \hat{x}(\nu + 1) - \frac{1}{3} \hat{x}(\nu) + \frac{2}{3} \Delta t \hat{f}(\nu + 2)
\end{equation*}

\subsection{Heun}
\begin{equation*}
	\hat{x}^{[P]}(\nu + 1) = \hat{x}(\nu) + \Delta t \hat{f}(\nu, \hat{x}(\nu))
\end{equation*}
\begin{equation*}
	\hat{x}(\nu + 1) = \hat{x}(\nu) + \frac{\Delta t}{2} \left( \hat{f}(\nu, \hat{x}(\nu)) + \hat{f}(\nu + 1, \hat{x}^{[P]}(\nu + 1)) \right)
\end{equation*}

\subsection{k-Schritt-Adams-Bashforth}
\begin{equation*}
	\hat x(\nu + k) = \hat x(\nu + k - 1) + \Delta t\sum_{i=0}^{k-1}b_{k,i}\hat f(\nu + i)
\end{equation*}
\begin{tabularx}{\columnwidth}{C|CCCCC}
$b_{i,k}$ & $i=0$ & $i=1$ & $i=2$ & $i=3$ \\ \hline
$k=1$ & $1$ & & & \\
$k=2$ & $-\frac{1}{2}$ & $\frac{3}{2}$ & & \\
$k=3$ & $\frac{5}{12}$ & $-\frac{16}{12}$ & $\frac{23}{12}$ & \\
$k=4$ & $-\frac{9}{24}$ & $\frac{37}{24}$ & $-\frac{59}{24}$ & $\frac{55}{24}$
\end{tabularx}
\end{sectionbox}

\section{Matlab Sample Code}
\begin{lstlisting}
function x = gaussVerfahren(A, b)
    [L, U, P] = LUZerlegung(A);
    [y] = vorwaertsSubstitution(L, P, b);
    [x] = rueckwaertsSubstitution(U, y);
end
\end{lstlisting}

\begin{lstlisting}
function [L, U, P] = LUZerlegung(A)
    n = size(A, 1);
    L = zeros(n, n);
    P = eye(n);

    for i = 1:n-1
        [pivot, pivotIndex] = max(abs(A(i:n, i)));
        pivotIndex = pivotIndex + (i - 1);
        pivot = A(pivotIndex, i);
        Psub = eye(n);
        Psub(:, [i, pivotIndex]) = Psub(:, [pivotIndex, i]);
        A([i, pivotIndex], :) = A([pivotIndex, i], :);
        L([i, pivotIndex], :) = L([pivotIndex, i], :);
        P = Psub*P;
        pivotRow = A(i, i+1:n);
        for j = i+1:n
            factor = A(j, i)/pivot;
            L(j, i) = factor;
            currentRow = A(j, i+1:n);
            A(j, i+1:n) = currentRow - factor*pivotRow;
            A(j, i) = 0;
        end
    end

    U = A;
    L = L + eye(n);
end
\end{lstlisting}

\begin{lstlisting}
function [y] = vorwaertsSubstitution(L, P, b)
    n = size(L, 1);
    y = zeros(n, 1);
    b = P*b;
    y(1) = b(1)/L(1, 1);

    for i = 2:n
        rowSum = L(i, 1:i-1)*y(1:i-1);
        y(i) = (b(i) - rowSum)/L(i, i);
    end
end
\end{lstlisting}

\begin{lstlisting}
function [x] = rueckwaertsSubstitution(U, y)
    n = size(U, 1);
    x = zeros(n, 1);
    x(n) = y(n)/U(n, n);

    for i = n-1:-1:1
        rowSum = U(i, i+1:n)*x(i+1:n);
        x(i) = (y(i) - rowSum)/U(i, i);
    end
end
\end{lstlisting}

\begin{lstlisting}
function [ x_k,r_k,alpha_k ] = conjugateGradientIteration( A,b,x0,N )
    x_k = zeros(length(x0),N+1);
    r_k = zeros(length(x0),N+1);
    p_k = zeros(length(x0),N+1);

    alpha_k = zeros(1,N);
    beta_k = zeros(1,N);

    x_k(:,1) = x0;
    r_k(:,1) = b-A*x0;
    p_k(:,1) = r_k(:,1);

    for i = 1:N
        Ap = A*p_k(:,i);
        alpha_k(i) = (p_k(:,i)'*r_k(:,i))./(p_k(:,i)'*Ap);
        x_k(:,i+1) = x_k(:,i) + alpha_k(i).*p_k(:,i);
        r_k(:,i+1) = r_k(:,i) -alpha_k(i).*Ap;
        beta_k(i) = (Ap'*r_k(:,i+1))./(Ap'*p_k(:,i));
        p_k(:,i+1) = r_k(:,i+1) - beta_k(i).*p_k(:,i);
    end
end
\end{lstlisting}

\begin{lstlisting}
function [x_k,r_k,alpha_k] = gradientIteration(A,b,x0,N)
    x_k = zeros(length(x0),N+1);
    r_k = zeros(length(x0),N);
    alpha_k = zeros(1,N);

    x_k(:,1) = x0;
    for i = 1:N
        r_k(:,i) = b - A*x_k(:,i);
        alpha_k(i) = (r_k(:,i)'*r_k(:,i))./(r_k(:,i)'*A*r_k(:,i));
        x_k(:,i+1) = x_k(:,i) + alpha_k(i).*r_k(:,i);
    end
end
\end{lstlisting}

\begin{lstlisting}
function [Q, R] = householder(A)
    n = size(A, 1);
    identity = eye(n);
    Q = eye(n);

    for i=1:(n-1)
        a = zeros(n, 1);
        a(i:end) = A(i:end, i);
        v = a + sign(a(i))*norm(a)*identity(:, i);
        Qpartial = identity - 2/(v'*v)*(v*v');
        Q = Qpartial*Q;
        A = Qpartial*A;
    end

    R = A;
    Q = Q';
end
\end{lstlisting}

\begin{lstlisting}
function [Q, R] = givensRotation(A)
    n = size(A, 1);
    Q = eye(n);
    R = A;

    for i = 1:(n-1)
        for j = i+1:n;
            G = createGivensRotation(R, j, i);
            Q = G*Q;
            R = G*R;
        end
    end

    Q = Q';
end
\end{lstlisting}

\begin{lstlisting}
function [G] = createGivensRotation(A, row, col)
    a1 = A(col, col);
    a2 = A(row, col);
    p = sqrt(a1*a1 + a2*a2);
    c = a1/p;
    s = a2/p;
    G = eye(size(A, 1));
    G(row, row) = c;
    G(col, col) = c;
    G(row, col) = (-1)*s;
    G(col, row) = s;
end
\end{lstlisting}

\begin{lstlisting}
function [a,b,c,d] = splineParameter(xi,f)
	n = max(size(xi));% Anzahl der Stuetzstellen
	a = f(xi);
    h = zeros(n-1,1);% Schrittweite

    for i=1:n-1
       h(i) = xi(i+1)-xi(i);
    end
    A = sparse(zeros(n,n));% Matrix fuer LGS
    bs = zeros(n,1);% rechte Seite fuer LGS
    for i=2:n-1
       A(i,i) = 2*(h(i)+h(i-1));
       A(i,i-1) = h(i-1);
       A(i,i+1) = h(i);
       bs(i) = (3/h(i))*(a(i+1)-a(i)) - (3/h(i-1))*(a(i)-a(i-1));
    end
    A(1,1) = 1;
    A(n,n) = 1;
    c = A\bs;% Loesung des LGS
    b = zeros(n,1);% Parameter b fuer Splines
    d = zeros(n,1);% Parameter d fuer Splines
    for i=1:n-1
        b(i) = (1/h(i))*(a(i+1)-a(i))-(h(i)/3)*(2*c(i)+c(i+1));
        d(i) = (1/(3*h(i)))*(c(i+1)-c(i));
    end
end
\end{lstlisting}

\section{Blabla Fragen}
\begin{sectionbox}

\begin{enumerate}
	\item \textbf{Nennen Sie einen Vorteil der Dividierten Differenzen gegenüber der Lagrange-Interpolation.}
	\begin{itemize}
		\item geringerer Aufwand
		\item keine komplette Neuberechnung bei neuer Stützstelle
	\end{itemize}
	\item \textbf{Nennen Sie einen Nachteil der Polynominterpolation gegenüber der Spline-Interpolation.}
	\begin{itemize}
		\item Oszillation am Intervallrand $\Rightarrow$ großer Fehler am Rand
	\end{itemize}

	\item \textbf{Nennen Sie zwei Vorteile des Adams-Bashfort-3-Schrittverfahrens gegenüber der Trapez- Methode zum Lösen nichtlinearer Differentialgleichungen.}
	\begin{itemize}
		\item höhere Genauigkeit (lokaler Fehler kleiner bei gleicher Schrittweite)
		\item explizites Verfahren (geringerer Rechenaufwand)
	\end{itemize}

	\item \textbf{Nennen Sie zwei Vorteile des Gauß-Verfahrens gegenüber dem Jacobi-Verfahren.}
	\begin{itemize}
		\item für alle nicht-singulären Matrizen lösbar
		\item geringerer Aufwand, wenn das gleiche Gleichungssystem mit verschiedenen rechten Seiten gelöst werden soll.
	\end{itemize}

	\item \textbf{Nennen Sie drei numerische Integrationsverfahren, die die gleiche (lokale) Fehlerordnung wie das Trapezverfahren besitzen.}
	\begin{itemize}
		\item Gear
		\item Taylor-Verfahren zweiter Ordnung
		\item Zweischritt Adams Bashfort
	\end{itemize}

	\item \textbf{Nennen Sie einen Vorteil des Jacobi-Verfahrens gegenüber dem Gauß-Seidel-Verfahren.}
	\begin{itemize}
		\item leicht parallelisierbar
	\end{itemize}

	\item \textbf{Nennen Sie einen Nachteil des Jacobi-Verfahrens gegenüber dem Gauß-Seidel-Verfahren.}
	\begin{itemize}
		\item langsamere Konvergenz
	\end{itemize}

	\item \textbf{Geben Sie an, welche numerischen Probleme bei Anwendung der Sekantenmethode zur Bestimmung der Nullstelle von $F(x)$ in der Nähe der Nullstelle $x_0$ auftreten können.}
	\begin{itemize}
		\item In der Nähe der Nullstelle ist $F(x^{(k)}) \approx 0$, weshalb in der Iterationsvorschrift näherungsweise der Term $\frac{0}{0}$ auftreten kann. Dementsprechend können Auslöschungsfehler auftreten.
	\end{itemize}

%	\item \textbf{Geben Sie die Determinante und Konditionszahl einer Givens-Rotationsmatrix G(l, k, θ) für eine beliebige Matrix A^Rm×m an. Begründen Sie.}
%	\item \textbf{Das Integral einer beliebig oft differenzierbaren Funktion wird mit der zusammengesetzten Trapezregel approximiert. Um welchen Faktor ändert sich der Approximationsfehler, wenn bei der zusammengesetzten Trapezregel die Anzahl der Teilintervalle verdoppelt wird?}
%	\item \textbf{}
\end{enumerate}

\end{sectionbox}











\end{document}





\begin{sectionbox}
	\subsection[Matrizen]{Matrizen $\ma A \in\mathbb{K}^{m \times n}$}
	\begin{tabularx}{\columnwidth}{LX}
	$(\ma A + \ma B)^\top = \ma A^\top + \ma B^\top$ & $(\ma A \cdot \ma B)^\top = \ma B^\top \cdot \ma A^\top$\\
	${(\ma A^\top)}^{-1} = {(\ma A^{-1})}^\top$ & $(\ma A \cdot \ma B)^{-1} = \ma B^{-1}\ma A^{-1}$
	\end{tabularx}

	\subsubsection{Dimensionen}

	\begin{tablebox}{ll}
	Bildraum & Nullraum \\ \mrule
	$\Bild \ma A = \iset{\ma A \vec x}{\vec x \in \K^n }$ & $\ker\ma A = \iset{\vec x \in \K^n}{\ma A \vec x = \vec 0}$\\
	$\rang \ma A = \dim(\Bild \ma A)$ & $\defect \ma A = \dim(\ker \ma A)$\\
	\end{tablebox}
	$\rang \ma A = r$ ist Anzahl. lin. unab. Spaltenvektoren.\\
	$\ma A$ erzeugt $\mathbb K \Leftrightarrow r = n$ \qquad $\ma A$ ist Basis von $\mathbb K \Leftrightarrow r = n = m$\\
	$\dim \mathbb K = n = \rang\ma A + \dim\ker\ma A$ \qquad $\rang\ma A = \rang\ma A^\top$



	\subsubsection{Quadratische Matrizen $A \in \mathbb{K}^{n \times n}$}
	regulär/invertierbar/nicht-singulär $\Leftrightarrow \det (\ma A) \ne 0 \Leftrightarrow \rang\ma A = n$\\
	singulär/nicht-invertierbar $\Leftrightarrow \det (\ma A) = 0 \Leftrightarrow \rang\ma A \ne n$\\
	orthogonal $\Leftrightarrow \ma A^\top=\ma A^{-1} \Ra \det(\ma A) = \pm 1$\\
	symmetrisch: $\ma A=\ma A^\top$ \qquad schiefsymmetrisch: $\ma A=-\ma A^\top$
	%\item hermitsch: $\ma A=\overline{\ma A}^\top$, unitär:$\ma A^{-1} = \overline{\ma A}^\top$
	

	\subsubsection[Determinante]{Determinante von $\ma A\in \mathbb K^{n\times n}$: $\det(\ma A)=|\ma A|$}
	$\det\mat{ \ma A & \ma 0 \\ \ma C& \ma D }= \det\mat{ \ma A & \ma B \\ \ma 0 & \ma D } = \det(\ma A)\det(\ma D)$ \\
	\begin{tabular*}{\columnwidth}{@{\extracolsep\fill}ll}
	$\det(\ma A) = \det(\ma A^T)$ & $\det(\ma A^{-1}) = \det(\ma A)^{-1}$
	\end{tabular*}
	$\det(\ma A\ma B) = \det(\ma A)\det(\ma B) = \det(\ma B)\det(\ma A) = \det(\ma B\ma A)$\\
	Hat $\ma A$ 2 linear abhäng. Zeilen/Spalten $\Rightarrow |\ma A|=0$ \\
	Entwicklung. n. $j$ter Zeile: $|\ma A|=\sum\limits_{i=1}^n (-1)^{i+j} \cdot a_{ij} \cdot |\ma A_{ij}|$\\

	\subsubsection{Eigenwerte $\lambda$ und Eigenvektoren $\underline v$}
	\framebox[\columnwidth]{\large $\ma A \vec v = \lambda \vec v$ \qquad $\det \ma A = \prod \lambda_i$ \qquad $\Sp \ma A = \sum a_{ii} = \sum \lambda_i$}\\
	Eigenwerte: $\det(\ma A - \lambda \ma 1) = 0$ Eigenvektoren: $\ker(\ma A - \lambda_i \ma 1) = \vec v_i$\\
	EW von Dreieck/Diagonal Matrizen sind die Elem. der Hauptdiagonale.


	\subsubsection{Spezialfall $2 \times 2$ Matrix $A$}
	\parbox{3cm}{ $\det(\ma A) = ad-bc$ \\ $\Sp(\ma A) = a+d$ } $\mat{a & b\\ c & d}^{-1} = \frac{1}{\det \ma A} \mat{d & -b\\ -c& a}$\\
	$\lambda_{1/2} = \frac{\Sp \ma A}{2} \pm \sqrt{ \left( \frac{\mathrm{sp} \ma A}{2} \right)^2 - \det \ma A }$


	\subsubsection{Spezielle Matrizen}
	Diagonalmatrix $\ma D$:  $\det \ma D = \prod d_{i}$\\
	$\ma D^{-1} =\operatorname{diag} \left(d_1, \dots, d_n\right)^{-1} = \operatorname{diag} \left(d_1^{-1}, \dots, d_n^{-1}\right)$\\
\end{sectionbox}


\begin{sectionbox}
	\subsection{Algebraische Strukturen $S(*)$}
	Menge $S$ mit binären Verknüpfungen $*:S\times S \rightarrow S$, deren Elemente bestimmte Axiome erfüllen ($a,b,c \in S$):
	\begin{itemize}\itemsep0pt
		\item Assoziativ: $(a* b) * c = a * (b * c)$
		\item Neutrales Element: $\forall a \exists_1 e \in S: a * e = e * a = a$ % \quad (meist $0$ oder $1$)
		\item Inverses Element: $\forall a \exists_1 a^{-1} \in S: a * a^{-1} = e$
		\item Kommutativ: $a * b = b * a$
		\item Distributiv: $a \cdot (b+c)=a\cdot b + a \cdot c$		
	\end{itemize}

	\begin{tablebox}{ll}
		Struktur & Definition \\ \cmrule
		Halbgruppe $(S,*)$ & $*$ ist assoziativ \\
		Monoid $(S,*)$ & Halbgruppe mit neutralem Element $e$\\
		Gruppe $(S,*)$ & Monoid mit Inversem $^{-1}$\\
		Abelsche Gruppe $(S,*)$ & Gruppe, so dass $*$ kommutativ ist.\\
		Ring $(S,+,\cdot)$ & abelsche Gruppe $(S,+)$, \\ & Halbgruppe $(S,\cdot)$, Distributivgesetz \\
		Körper $(S,+,\cdot)$ & Ring mit abelscher Gruppe $(S\setminus\{ 0 \}, \cdot)$\\
	\end{tablebox}
\end{sectionbox}



\begin{sectionbox}
	\subsection{Mengen}
	% =============================================================================================
	Eine Menge ist eine Zusammenfassung bestimmter, wohlunterschiedener Elemente zu einem Ganzen.
	Es gibt zwei Möglichkeiten eine Menge zu definieren:\\
	1. Angabe durch explizite Aufzählung der Elemente: $A=\{1;2;3\}$\\
	2. Angabe von Eigenschaften aller Elemente: $A=\{n\in\mathbb N\ \vert\ 0<n<4\}$\\
	
		\subsection{Wichtige Mengen}
		\begin{tablebox}{rll}
			$\emptyset$ & $=\{ \}$ & Leere Menge\\
			$\mathbb N$ & $=\{1,2,3,4,5,\dotsc\}$ & Natürliche Zahlen \\
			$\mathbb Z$ & $=\{\dotsc ,-2,-1,0,1,2,\dotsc\}$ & Ganze Zahlen \\
			$\mathbb Q$ & $=\iset {\frac m n}{  m \in \mathbb Z, n \in \mathbb N }$ & Rationale Zahlen\\
			$\mathbb R$ & $=\{\dotsc ,-\sqrt{2},\dotsc,0,\dotsc,\pi,\dotsc\}$ & Reele Zahlen\\	
			$\mathbb C$ & $=\mathbb R^2$ & Komplexe Zahlen\\
			$\mathbb B$ & $=\{ 0,1 \}$ & Binäre Zahlen\\		
			$\mathbb S$ & $=\{ -1,0,1 \}$ & Vorzeichenmenge\\	
			$\mathcal K[x]$ & & Polynomring\\ 		
		\end{tablebox}
\end{sectionbox}






\begin{sectionbox}
\subsubsection{Matrixnorm}
\begin{tabularx}{\columnwidth}{LX}
	$\norm{\ma{A}} = 0 \Rightarrow \ma{A} = 0$ & $\norm{\alpha \ma{A}} = \abs{\alpha} \norm{\ma{A}}$\\
	$\norm{\ma{A} + \ma{B}} \le \norm{\ma{A}} + \norm{\ma{B}}$ & $\norm{\ma{A} \cdot \ma{B}} \le \norm{\ma{A}} \cdot \norm{\ma{B}}$
\end{tabularx}
\begin{tabular}{@{}r@{ = }ll}
	$\norm{\ma{A}}_M$ & $n\cdot\underset{i,j}{\max}\abs{a_{ij}}$ & Gesamtnorm, Matrixnorm\\
	$\norm{\ma{A}}_\infty$ & $\underset{i}{\max}\sum\limits_{j=1}^n\abs{a_{ij}}$ & Zeilennorm (max Zeilensumme)\\
	$\norm{\ma{A}}_1$ & $\underset{j}{\max}\sum\limits_{i=1}^n\abs{a_{ij}}$ & Spaltennorm (max Spaltensumme)\\
	$\norm{\ma{A}}_E$ & $\sqrt{\sum\limits_{i=1}\sum\limits_{j=1}\abs{a_{ij}}^2}$ & $\underset{\text{euklidische Norm}}{\text{Frobeniusnorm}}$ $(||\ma{1}||_E = \sqrt{n})$\\
	$\norm{\ma{A}}_\lambda$ & $\sqrt{\lambda_\text{max}(\ma{A}^T\cdot\ma{A})}$ & Spektralnorm, Hilbertnorm \\
\end{tabular}


\end{sectionbox}